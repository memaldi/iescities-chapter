\section{Identified challenges}

Taking into consideration the large amounts of data present at smart cities, data management's complexity can be described in terms of:
\begin{itemize}
	\item Volume
	\item Variety
	\item Veracity
	\item Velocity
\end{itemize}

This four variables can also be found in \textit{Big Data}-related articles (also known as the \textit{Big Data's Vs}) \cite{zikopoulos2011understanding,russom2011big}, so it is not surprising at all that smart cities are going to deal with Big Data problems in the near future (if they are not dealing with them right now).

Data scientist need to take into account these variables, which could overlap in certain enviroments. Should this happen, each scenario will determine the most relevant factors of the process, generating unwelcome drawbacks on the other ones.

\subsection{Volume}

The high amount of data generated and used by cities nowadays needs to be properly analysed, processed, stored and eventually accessible. This means conventional IT structures need to evolve, enabling scalable storage technologies, distributed querying approaches and massively parallel processing algorithms and architectures.

There is a growing trend which defends that the minimum amount of data should be stored and analysed without significantly affecting the overall knowledge that could be extracted from the whole dataset. Based on \textit{Pareto's Principle} (also known as the 80-20 rule), the idea is to focus on a 20\% of the data to be able to extract up to the 80\% of knowledge within it. Even being a solid research challenge in Big Data, there are ocassions where we can not discard data from being stored or analysed (e.g., sensor data about monitoring a building can be used temporally, while patient monitoring data should be kept for historical records).

% Apache Hadoop, MapReduce, No-SQL...

However, big amounts of data should not be seen as a drawback attached to smart cities. The larger the datasets, the better analysis algorithms can perform, so deeper insights and conclusions should be expected as an outcome. These could ease the decission making stage.

As management consultant Peter Drucker once said: \textit{``If you can not measure it, you can not manage it''}, thus leaving no way to improve it either. This adage manifests that should you want to take care of some process, but you are not able to measure it or you can not access the data, you will not be able to manage that process. That being said, the higher amounts of data available, the greater the opportunities of obtaining useful knowledge will become.

\subsection{Variety}

Data is rarely found in a perfectly ordered and ready for processing format. Data scientists are used to work with diverse sources, which seldom fall into neat relational structures: embedded sensor data, documents, media content, social data, etc. As can be seen at section \ref{sec:capture} there are many different sources where data can come from in a smart city. Despite of presented data cycle can be applied for all kind of data, the different steps of the cycle have to be planificated, avoiding the overloading of implemented system. For example, data from social media talking about an emergency situation may be prioritised over the rest of the data, to allow Emergency Response Teams (ERTs) react as soon as possible.

Moreover, different data sources can describe the same real world entities in such different ways, finding conflicting information, different data-types, etc. Taking care of how data sources describe their contents will lead to an easier integration step, lowering development, analytics and maintenance costs over time.

\subsection{Veracity}

There is also an increasing concern on data trustworthiness. Different data sources can have meaningful differences in terms of quality, coverage, accuracy, timeliness and consistency of the provided data. In fact, \cite{xian_truth_2013} conclude that redundancy, consistency, correctness and copying between sources are the most recurrent issues we have to deal with when we try to find trustworthy information from a wide variety of heterogeneous sources.

As pointed out by \cite{buneman2013data}, \textit{data provenance is fundamental to understanding data quality}. They also highlight that established information storage systems may not be adecuate to keep semantic sense of data.

Several efforts are trying to convert existing data in high quality data, providing an extra confidence layer in which data analysts can rely. In a previous research \cite{emalditrust}, we introduced a provenance data model to be used in user-generated Linked Data datasets, which follow W3C's PROV-O ontology\footnote{http://www.w3.org/TR/prov-o/}. Some other researchs as \cite{hartig_using_2009} or \cite{bizer_quality_2009} also provide some mechanisms to measure the quality and trust in Linked Data.

\subsection{Velocity}

Finally, we must assume that data generation is experiencing an exponential growth. That forces our IT structure to not only tackle with volume issues, but with high processing rates. A widely spread concept among data businesses is that sometimes you can not rely on five-minute-old data for your business logic.

That is why \textit{streaming data} has moved from academic fields to industry to solve velocity problems. There are two main reasons to consider streaming processing:
\begin{itemize}
	\item Sometimes, input data is too fast to store in their entirety without rocketing costs.
	% At the extreme end of the scale, the Large Hadron Collider at CERN generates so much data that scientists must discard the overwhelming majority of it — hoping hard they’ve not thrown away anything usefu
	\item If applications mandate immediate response to the data, batch processes are not suitable. Due to the rise of smartphone applications, this trend is increasingly becoming a common scenario.
\end{itemize}
